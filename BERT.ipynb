{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67650669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create -n env_pytorch python=3.6\n",
    "#conda activate env_pytorch\n",
    "#pip install torchvision\n",
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97cad69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 73106\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import transformers\n",
    "\n",
    "# Step 2: Import the IMDB data set and preprocess it\n",
    "def load_data(data_file):\n",
    "    # Read the Excel file, explicitly specifying the engine\n",
    "    df = pd.read_excel(data_file, engine='openpyxl')  # Use the 'openpyxl' engine for Excel files\n",
    "    texts = df['Complaint'].tolist()\n",
    "    labels = df['Category Level 1'].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "# Load the data\n",
    "documents_path = os.path.expanduser('~\\\\Documents')\n",
    "os.chdir(documents_path)\n",
    "data_file = 'final_data.xlsx'\n",
    "texts, labels = load_data(data_file)\n",
    "\n",
    "# Reduce dataset size to 10% for easier computation\n",
    "subset_ratio = 1\n",
    "subset_size = int(len(texts) * subset_ratio)\n",
    "texts = texts[:subset_size]\n",
    "labels = labels[:subset_size]\n",
    "\n",
    "# Print dataset size\n",
    "print(f\"Number of samples in the dataset: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a821f24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length:  592.7580773123957\n",
      "Mean length:  379.0\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create a custom dataset class for text classification\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Ensure text is a string before tokenization\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)  # Convert to string if it's not\n",
    "\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "# Step 4: Build our custom BERT classifier\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "# Step 5: Define training function\n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# Step 6: Build our evaluation method\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    report = classification_report(actual_labels, predictions)\n",
    "    return accuracy, report, actual_labels, predictions\n",
    "\n",
    "import numpy as np\n",
    "mean_len = np.mean([len(comp) for comp in texts])\n",
    "median_len = np.median([len(comp) for comp in texts])\n",
    "print('Mean length: ', mean_len)\n",
    "print('Mean length: ', median_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf65670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                   | 2/3656 [03:17<100:21:43, 98.88s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Define our modelâ€™s parameters\n",
    "# Set up parameters\n",
    "bert_model_name = 'indobenchmark/indobert-base-p1'\n",
    "num_classes = 18\n",
    "max_length = 300\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Step 9: Loading and splitting the data.\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=123)\n",
    "\n",
    "# Step 10: Initialize tokenizer, dataset, and data loader\n",
    "# Encode labels BEFORE creating datasets\n",
    "le = LabelEncoder()\n",
    "train_labels_encoded = le.fit_transform(train_labels)\n",
    "val_labels_encoded = le.transform(val_labels)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels_encoded, tokenizer, max_length)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels_encoded, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Step 11: Set up the device and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(bert_model_name, num_classes=num_classes).to(device)\n",
    "\n",
    "# Step 12: Set up optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train(model, train_dataloader, optimizer, scheduler, device)\n",
    "    accuracy, report, actual_labels, predictions = evaluate(model, val_dataloader, device)\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(report)\n",
    "    print(f\"Epoch {epoch + 1} processing time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"bert_classifierall_1408.pth\")\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for BERT\")\n",
    "plt.show()\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(f\"runtime: {runtime:.2f} seconds\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have 'actual_labels' and 'predictions' from the previous run\n",
    "# Convert numeric labels back to category names\n",
    "actual_labels_names = le.inverse_transform(actual_labels)\n",
    "predictions_names = le.inverse_transform(predictions)\n",
    "\n",
    "# Generate and print the new classification report\n",
    "new_report = classification_report(actual_labels_names, predictions_names)\n",
    "print(new_report)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"bert_classifierall_0308.pth\")\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greys\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for BERT\")\n",
    "plt.show()\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(f\"runtime: {runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b244ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create -n env_pytorch python=3.6\n",
    "#conda activate env_pytorch\n",
    "#pip install torchvision\n",
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53c303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
